From 418ade7849ce7641c0f7333718caf5091a02fd4c
Nicolas Guibourge <nicolasg@microsoft.com>, Wed Aug 24 2022, back port of CVE-2022-35414 to version 4.2.0

diff -ru qemu-4.2.0-orig/exec.c qemu-4.2.0/exec.c
--- qemu-4.2.0-orig/exec.c	2019-12-12 10:20:47.000000000 -0800
+++ qemu-4.2.0/exec.c	2022-08-24 02:18:22.054019514 -0700
@@ -685,7 +685,7 @@
 
 /* Called from RCU critical section */
 MemoryRegionSection *
-address_space_translate_for_iotlb(CPUState *cpu, int asidx, hwaddr addr,
+address_space_translate_for_iotlb(CPUState *cpu, int asidx, hwaddr orig_addr,
                                   hwaddr *xlat, hwaddr *plen,
                                   MemTxAttrs attrs, int *prot)
 {
@@ -694,6 +694,7 @@
     IOMMUMemoryRegionClass *imrc;
     IOMMUTLBEntry iotlb;
     int iommu_idx;
+    hwaddr addr = orig_addr;
     AddressSpaceDispatch *d = atomic_rcu_read(&cpu->cpu_ases[asidx].memory_dispatch);
 
     for (;;) {
@@ -737,6 +738,16 @@
     return section;
 
 translate_fail:
+    /*
+     * We should be given a page-aligned address -- certainly
+     * tlb_set_page_with_attrs() does so.  The page offset of xlat
+     * is used to index sections[], and PHYS_SECTION_UNASSIGNED = 0.
+     * The page portion of xlat will be logged by memory_region_access_valid()
+     * when this memory access is rejected, so use the original untranslated
+     * physical address.
+     */
+    assert((orig_addr & ~TARGET_PAGE_MASK) == 0);
+    *xlat = orig_addr;
     return &d->map.sections[PHYS_SECTION_UNASSIGNED];
 }
 #endif
